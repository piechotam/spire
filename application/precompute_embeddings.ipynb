{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jaxtyping'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcept_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcept_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_image_from_datasets, extract_embedding\n\u001b[32m      7\u001b[39m folder_path = \u001b[33m\"\u001b[39m\u001b[33mstl10_sample_500\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m ViT_B_16_clip, image_transform = load(\u001b[33m\"\u001b[39m\u001b[33mViT-B/16\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/spire/concept_extraction/concept_extraction.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msparse_autoencoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoencoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseAutoencoder\n\u001b[32m     11\u001b[39m _SAE_N_INPUT_FEATURES = \u001b[32m512\u001b[39m\n\u001b[32m     12\u001b[39m _SAE_CONCEPTS_SCALING = \u001b[32m8\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/spire/sparse_autoencoder/autoencoder/model.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThe Sparse Autoencoder Model. \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mCode from https://github.com/neuroexplicit-saar/Discover-then-Name/blob/main/sparse_autoencoder/sparse_autoencoder/autoencoder/model.py\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m final\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjaxtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Float\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositiveInt, validate_call\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jaxtyping'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "from clip.clip import load\n",
    "from concept_extraction.concept_extraction import prepare_image_from_datasets, extract_embedding\n",
    "\n",
    "folder_path = \"stl10_sample_500\"\n",
    "ViT_B_16_clip, image_transform = load(\"ViT-B/16\")\n",
    "\n",
    "image_paths = glob.glob(folder_path + \"/*.png\")\n",
    "samples = []\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    samples.append({\"image\": img})\n",
    "\n",
    "embeddings_list = []\n",
    "for sample in samples:\n",
    "    _, image_transformed = prepare_image_from_datasets(sample[\"image\"], image_transform)\n",
    "    embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "embeddings = torch.stack(embeddings_list)\n",
    "\n",
    "torch.save({\n",
    "    \"embeddings\": embeddings,\n",
    "    \"image_paths\": image_paths,\n",
    "}, \"precomputed_embeddings.pt\")\n",
    "\n",
    "print(f\"Saved {len(samples)} embeddings to precomputed_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "from clip.clip import load\n",
    "from concept_extraction.concept_extraction import prepare_image_from_datasets, extract_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"stl10_sample_500\"\n",
    "ViT_B_16_clip, image_transform = load(\"ViT-B/16\")\n",
    "\n",
    "image_paths = glob.glob(folder_path + \"/*.png\")\n",
    "samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    samples.append({\"image\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zuzannasienko/Documents/github/spire\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stl10_sample_500'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(os.getcwd())  \u001b[38;5;66;03m# aktualny katalog roboczy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstl10_sample_500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# co jest w folderze?\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'stl10_sample_500'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # aktualny katalog roboczy\n",
    "print(os.listdir(\"stl10_sample_500\"))  # co jest w folderze?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n\u001b[32m     14\u001b[39m     embeddings_list.append(embedding)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m embeddings = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m torch.save({\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m: embeddings,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_paths\u001b[39m\u001b[33m\"\u001b[39m: image_paths,\n\u001b[32m     21\u001b[39m }, \u001b[33m\"\u001b[39m\u001b[33mprecomputed_embeddings.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m embeddings to precomputed_embeddings.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    samples.append({\"image\": img})\n",
    "\n",
    "embeddings_list = []\n",
    "for sample in samples:\n",
    "    _, image_transformed = prepare_image_from_datasets(sample[\"image\"], image_transform)\n",
    "    embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "embeddings = torch.stack(embeddings_list)\n",
    "\n",
    "torch.save({\n",
    "    \"embeddings\": embeddings,\n",
    "    \"image_paths\": image_paths,\n",
    "}, \"precomputed_embeddings.pt\")\n",
    "\n",
    "print(f\"Saved {len(samples)} embeddings to precomputed_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = []\n",
    "for i, sample in enumerate(samples):\n",
    "    _, image_transformed = prepare_image_from_datasets(sample[\"image\"], image_transform)\n",
    "    embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n",
    "    print(f\"Embedding {i}: type={type(embedding)}, shape={embedding.shape if embedding is not None else None}\")\n",
    "    embeddings_list.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _, image_transformed = prepare_image_from_datasets(\u001b[43msample\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m], image_transform)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(image_transformed), image_transformed.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "_, image_transformed = prepare_image_from_datasets(sample[\"image\"], image_transform)\n",
    "print(type(image_transformed), image_transformed.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
