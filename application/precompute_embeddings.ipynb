{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from clip.clip import load\n",
    "from concept_extraction.concept_extraction import (\n",
    "    prepare_image_from_datasets,\n",
    "    extract_embedding,\n",
    "    extract_sae_activations,\n",
    "    load_sae\n",
    ")\n",
    "\n",
    "\n",
    "STL10_FOLDER = Path(\"stl10_sample_500\")\n",
    "SAE_CHECKPOINT = Path(\"..\") / \"sae_checkpoints\" / \"clip_ViT-B_16_sparse_autoencoder_final.pt\"\n",
    "OUTPUT_CLIP = Path(\"stl10_embeddings.pt\")\n",
    "OUTPUT_SAE = Path(\"stl10_sae_activations.pt\")\n",
    "\n",
    "def save_embeddings_and_sae_activations():\n",
    "    # Załaduj model CLIP i przekształcenia\n",
    "    ViT_B_16_clip, image_transform = load(\"ViT-B/16\")\n",
    "\n",
    "    # Załaduj sparse autoencoder\n",
    "    sparse_autoencoder = load_sae(SAE_CHECKPOINT)\n",
    "\n",
    "    # Wczytaj obrazy\n",
    "    image_paths = list(STL10_FOLDER.glob(\"*.png\"))[:200]\n",
    "    samples = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        samples.append({\"image\": img})\n",
    "\n",
    "    # Oblicz embeddingi CLIP\n",
    "    embeddings_list = []\n",
    "    for sample in samples:\n",
    "        image = sample[\"image\"]\n",
    "        _, image_transformed = prepare_image_from_datasets(image, image_transform)\n",
    "        embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "    embeddings = torch.stack(embeddings_list)\n",
    "\n",
    "    # Zapisz embeddingi\n",
    "    torch.save({\n",
    "        'embeddings': embeddings,\n",
    "        'image_paths': [str(p) for p in image_paths],\n",
    "    }, OUTPUT_CLIP)\n",
    "    print(f\"✅ Saved CLIP embeddings to {OUTPUT_CLIP}\")\n",
    "\n",
    "    # Oblicz aktywacje SAE\n",
    "    sae_activation_list = []\n",
    "    for embedding in embeddings_list:\n",
    "        activation = extract_sae_activations(embedding, sparse_autoencoder)\n",
    "        sae_activation_list.append(activation)\n",
    "\n",
    "    sae_activations = torch.stack(sae_activation_list)\n",
    "\n",
    "    # Zapisz aktywacje SAE\n",
    "    torch.save({\n",
    "        'sae_activations': sae_activations,\n",
    "        'image_paths': [str(p) for p in image_paths],\n",
    "    }, OUTPUT_SAE)\n",
    "    print(f\"✅ Saved SAE activations to {OUTPUT_SAE}\")\n",
    "\n",
    "# Uruchom zapis\n",
    "if __name__ == \"__main__\":\n",
    "    save_embeddings_and_sae_activations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from clip.clip import load\n",
    "from concept_extraction.concept_extraction import (\n",
    "    prepare_image_from_datasets,\n",
    "    extract_embedding,\n",
    "    extract_sae_activations,\n",
    "    load_sae\n",
    ")\n",
    "\n",
    "\n",
    "STL10_FOLDER = Path(\"stl10_sample_500\")\n",
    "SAE_CHECKPOINT = Path(\"..\") / \"sae_checkpoints\" / \"clip_ViT-B_16_sparse_autoencoder_final.pt\"\n",
    "OUTPUT_CLIP = Path(\"stl10_embeddings.pt\")\n",
    "OUTPUT_SAE = Path(\"stl10_sae_activations.pt\")\n",
    "\n",
    "def save_embeddings_and_sae_activations():\n",
    "    ViT_B_16_clip, image_transform = load(\"ViT-B/16\")\n",
    "    sparse_autoencoder = load_sae(SAE_CHECKPOINT)\n",
    "\n",
    "    image_paths = list(STL10_FOLDER.glob(\"*.png\"))[:200]\n",
    "    samples = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        samples.append({\"image\": img})\n",
    "\n",
    "    embeddings_list = []\n",
    "    for sample in samples:\n",
    "        image = sample[\"image\"]\n",
    "        _, image_transformed = prepare_image_from_datasets(image, image_transform)\n",
    "        embedding = extract_embedding(image_transformed=image_transformed, clip_model=ViT_B_16_clip)\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "    embeddings = torch.stack(embeddings_list)\n",
    "\n",
    "    torch.save({\n",
    "        'embeddings': embeddings,\n",
    "        'image_paths': [str(p) for p in image_paths],\n",
    "    }, OUTPUT_CLIP)\n",
    "    print(f\"Saved CLIP embeddings to {OUTPUT_CLIP}\")\n",
    "\n",
    "    sae_activation_list = []\n",
    "    for embedding in embeddings_list:\n",
    "        activation = extract_sae_activations(embedding, sparse_autoencoder)\n",
    "        sae_activation_list.append(activation)\n",
    "\n",
    "    sae_activations = torch.stack(sae_activation_list)\n",
    "\n",
    "    torch.save({\n",
    "        'sae_activations': sae_activations,\n",
    "        'image_paths': [str(p) for p in image_paths],\n",
    "    }, OUTPUT_SAE)\n",
    "    print(f\"Saved SAE activations to {OUTPUT_SAE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_and_sae_activations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
